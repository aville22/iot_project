from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Any

import numpy as np
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
)


@dataclass
class EvalResult:
    """
    Container for evaluation outputs.
    """
    metrics: Dict[str, Any]
    confusion: np.ndarray


def evaluate_classifier(y_true: np.ndarray, y_pred: np.ndarray, labels: list[int]) -> EvalResult:
    """
    Compute standard classification metrics and confusion matrix.
    """
    acc = float(accuracy_score(y_true, y_pred))

    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, labels=labels, average=None, zero_division=0
    )

    macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="macro", zero_division=0
    )
    weighted_p, weighted_r, weighted_f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="weighted", zero_division=0
    )

    cm = confusion_matrix(y_true, y_pred, labels=labels)

    metrics = {
        "accuracy": acc,
        "per_class": {
            str(lbl): {
                "precision": float(precision[i]),
                "recall": float(recall[i]),
                "f1": float(f1[i]),
                "support": int(support[i]),
            }
            for i, lbl in enumerate(labels)
        },
        "macro_avg": {"precision": float(macro_p), "recall": float(macro_r), "f1": float(macro_f1)},
        "weighted_avg": {
            "precision": float(weighted_p),
            "recall": float(weighted_r),
            "f1": float(weighted_f1),
        },
    }

    return EvalResult(metrics=metrics, confusion=cm)
